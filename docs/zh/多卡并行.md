# 分布式训练

**注意:** 目前我们我们只测试了单机多卡的正确性。
- [分布式训练](#%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83)
  - [使用要求](#%E4%BD%BF%E7%94%A8%E8%A6%81%E6%B1%82)
  - [使用方法](#%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95)
  - [效果](#%E6%95%88%E6%9E%9C)
  - [Q&A](#qa)
    - [为什么我们不使用`DistributedDataParallel`](#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E4%BB%AC%E4%B8%8D%E4%BD%BF%E7%94%A8distributeddataparallel)
    - [为什么我们不使用`Horovod`](#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E4%BB%AC%E4%B8%8D%E4%BD%BF%E7%94%A8horovod)

## 使用要求

需要安装NCCL，可以到NVIDIA[官方网站](https://developer.nvidia.com/nccl)上下载并按照文档进行安装。

## 使用方法

我们采取一个launch脚本来启动分布式训练。比如，我们希望在一台具有四块GPU的服务器上进行训练，那么如下修改训练的脚本:

``` bash
python -m src.distributed.launch \
    --nproc_per_node=4 \
    --nnodes=1 --node_rank=0 \
    --master_addr="127.0.0.1" --master_port=1234 \
    src.bin.train \
    --model_name <your-model-name> \
    --reload \
    --config_path <your-config-path> \
    --log_path <your-log-path> \
    --saveto <path-to-save-checkpoints> \
    --valid_path <path-to-save-validation-translation> \
    --use_gpu \
    --shared_dir "/tmp"
```

其中，`nnodes`表明每个总共有一个节点(机器)，`nproc_per_node`表明这个节点上有4个进程(四块GPU，因为我们对于每块GPU的训练任务分配一个进程)，`node_rank`表明该节点的顺序在整个任务中属于0号节点(从0开始编号，因为是单机，所以自然是0)；`master_addr`和`master_port`分别表明主节点的ip地址和端口号，同样因为单机，所以ip地址设为localhost即可；`shared_dir`指定一个共享的路径，用于在进程之前传递一些较大的数据，具体方式为:

    1.将每个进程需要共享的数据在这个共享路径下保存为一些临时文件。
    2.在进程之间传递这些临时文件的路劲。
    3.读取，合并并还原出需要共享的数据

按照类似的方式，我们同样可以运行分布式解码。

## 效果

我们展示一个在IWSLT15 English-Vietnamese上的实验。batch size设为4096。在TITAN-X上，我们比较了单卡和双卡在速度和dev上损失函数的比较i。单卡的配置文件上设置为`batch_size=1024,update_cycle=4`。双卡的配置文件上设置为`batch_size=1024, update_cycle=2`。Tensorboard上速度和loss曲线的对比如下(蓝色为单卡，红色为双卡)：

速度对比

<img src="../distributed-speed-comp.png" width="300" height="200">

loss曲线对比

<img src="../distributed-loss-comp.png" width="300" height="200">


## Q&A

### 为什么我们不使用`DistributedDataParallel`

`DistributedDataParallel`(DDP)目前通过hook的方式来进程间梯度的通讯，即每一次求导后，DDP会立即开始执行allreduce操作，向其他交换梯度的信息。这是一种常见的合拼计算开销和通讯开销的技巧。然而，这样的实现没办法兼容`update_cycle`机制，即在将较大的batch切分成若干个小batch计算并累计梯度。此外，需要将程序中的每一个module转换成DDP类型的module，个人认为也较为不便。

为此，我们参考了Horovod的设计，将这些操作放在了optimizer的位置，并改为在执行梯度更新之前手动执行梯度的allreduce操作。这样我们可以执行module若干forward和backward操作，然后由optimizer完成参数更新。当然，这样会造成一些性能上的损失。

### 为什么我们不使用`Horovod`

使用Horovod需要安装OpenMPI，其安装时较为繁琐的(尤其在没有sudo权限的情况下)。此外，Horovod并不支持在windows上安装。

