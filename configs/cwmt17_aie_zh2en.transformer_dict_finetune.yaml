data_configs:
  lang_pair: "zh-en"
  train_data:
    - "/your/path/to/FGraDA/dict/education.dict.tok.zh"
    - "/your/path/to/FGraDA/dict/education.dict.tok.en"
  valid_data:
    - "/your/path/to/FGraDA/dev/education.dev.zh.tok"
    - "/your/path/to/FGraDA/dev/education.dev.en.tok"
  bleu_valid_reference: "/your/path/to/FGraDA/dev/education.dev.en"
  vocabularies:
    - type: "bpe"
      apply_mod: False
      dict_path: "/your/path/to/cwmt17_zh-en/zh.vocab.json"
      max_n_words: -1
      codes: "/your/path/to/cwmt17_zh-en/zh.codes"
    - type: "bpe"
      apply_mod: False
      dict_path: "/your/path/to/cwmt17_zh-en/en.vocab.json"
      max_n_words: -1
      codes: "/your/path/to/cwmt17_zh-en/en.codes"
  max_len:
    - -1
    - -1
  num_refs: 1
  eval_at_char_level: false

model_configs:
  model: Transformer
  n_layers: 6
  n_head: 8
  d_word_vec: 512
  d_model: 512
  d_inner_hid: 2048
  dropout: 0.1
  proj_share_weight: true
  label_smoothing: 0.1

optimizer_configs:
  optimizer: "adam"
  learning_rate: 0.0000005
  grad_clip: -1.0
  optimizer_params: ~
  schedule_method: ~
  # scheduler_configs:
  #   d_model: 512
  #   warmup_steps: 8000

training_configs:
  seed: 1234
  max_epochs: 100
  shuffle: true
  use_bucket: true
  batch_size: 100
  batching_key: "tokens"
  update_cycle: 2
  valid_batch_size: 20
  disp_freq: 4
  save_freq: 4
  num_kept_checkpoints: 2
  loss_valid_freq: 4
  bleu_valid_freq: 4
  bleu_valid_batch_size: 20
  bleu_valid_warmup: 1
  bleu_valid_configs:
    max_steps: 150
    beam_size: 5
    alpha: 0.0
    sacrebleu_args: "-lc"
    postprocess: True
  early_stop_patience: 50
